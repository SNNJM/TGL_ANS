{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Solution Architecture"
      ],
      "metadata": {
        "id": "tNoGmenRua3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│                         PDF Ingestion                           │\n",
        "│─────────────────────────────────────────────────────────────────│\n",
        "│ • Watch /content/data for new .pdf files                        │\n",
        "│ • Load each PDF with pdfplumber                                 │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "               ↓\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│                    Multimodal Text Extraction                   │\n",
        "│─────────────────────────────────────────────────────────────────│\n",
        "│ For each PDF page:                                              │\n",
        "│  1. Extract “plain” text                                        │\n",
        "│  2. Extract tables → pandas                                     │\n",
        "│  3. Render page to image + EasyOCR for scans                    │\n",
        "│ ⇒ Produce one raw‐text blob per PDF                            │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "               ↓\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│                       Document Chunking                         │\n",
        "│─────────────────────────────────────────────────────────────────│\n",
        "│ • Wrap each blob in a LangChain Document                        │\n",
        "│ • Split with RecursiveCharacterTextSplitter                     │\n",
        "│   (chunk_size=500, chunk_overlap=50)                            │\n",
        "│ ⇒ Yields ~N small “chunks” of text                             │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "               ↓\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│                        Vector Indexing                          │\n",
        "│─────────────────────────────────────────────────────────────────│\n",
        "│ • Embed each chunk with                                         │\n",
        "│   SentenceTransformerEmbeddings(all‑MiniLM)                     │\n",
        "│ • Build / load FAISS index                                      │\n",
        "│   – Save to “shipment_index” for persistence                    │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "               ↓\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│                         RAG Retrieval                           │\n",
        "│──────────────────────────────────────────────────────────────── │\n",
        "│ answer_with_rag(query):                                         │\n",
        "│  1. FAISS.similarity_search(query, k=1000)                      │\n",
        "│  2. Take top‑K chunks (metadata.source + text)                  │\n",
        "│  3. Concatenate into a “context” string                         │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "               ↓\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│                        LLM Reasoning                            │\n",
        "│──────────────────────────────────────────────────────────────── │\n",
        "│ • SystemMessage: “You are a shipping‑document analyst.”         │\n",
        "│ • HumanMessage:                                                 │\n",
        "│     – Paste context                                             │\n",
        "│     – “Group by consignor, document type…                       │\n",
        "│        respond exactly in this format:                          │\n",
        "│        I found <N> shipments FOR <X> …”                         │\n",
        "│ • Call Gemini via ChatGoogleGenerativeAI                        │\n",
        "│ • Parse AIMessage.content for final output                      │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "               ↓\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│                         End User                                │\n",
        "│─────────────────────────────────────────────────────────────────│\n",
        "│ “How many shipments are currently tracked?”                     │\n",
        "│ ⇒ Prints a clean, structured answer:                           │\n",
        "│    I found 17 shipments FOR …                                   │\n",
        "│       Shipment1: Reference: …                                   │\n",
        "│                   …etc.                                         │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "g9J9R-Xzc61Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to run this Colab\n",
        "\n",
        "1. Create a folder name data (the location would be /content/data)\n",
        "2. Upload the shipping pdfs into it\n",
        "3. set runtime to T4 GPU with high RAM and connect\n",
        "4. I set up hf token to Google Colab Secret, preferably set the Gemini API Key there too\n",
        "4. Run the cells below"
      ],
      "metadata": {
        "id": "FaJ9l9eje7ha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install required libraries"
      ],
      "metadata": {
        "id": "OSkpMBAsfAoE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk3rMEVqcW2H",
        "outputId": "9a997472-410b-4c7a-981b-5505971b76fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m338.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet\\\n",
        "    google-ai-generativelanguage==0.6.15 \\\n",
        "    google-generativeai==0.8.5 \\\n",
        "    pdfplumber \\\n",
        "    easyocr \\\n",
        "    pillow \\\n",
        "    pandas \\\n",
        "    langchain \\\n",
        "    sentence-transformers \\\n",
        "    faiss-cpu \\\n",
        "    langchain-google-genai \\\n",
        "    torch\\\n",
        "    langchain-community\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import required libraries"
      ],
      "metadata": {
        "id": "BFZuaPxofeoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pdfplumber\n",
        "import easyocr\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import SystemMessage, HumanMessage"
      ],
      "metadata": {
        "id": "MIZ7TYPmfZpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "wO5OVSx7f4CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PDF_DIRECTORY    = \"/content/data/\"\n",
        "VECTORSTORE_PATH = \"shipment_index\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDSmigHq-eJu3ezg_QAk-yxk0JFnlnVH4o\""
      ],
      "metadata": {
        "id": "DtVd_z-Nf0Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OCR - EasyOCR"
      ],
      "metadata": {
        "id": "pUGcJ3OCgWrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Initialize EasyOCR ---\n",
        "reader = easyocr.Reader(['en'], gpu=True)\n",
        "\n",
        "# --- Extract Text + Tables + OCR from PDFs ---\n",
        "def extract_text_from_pdfs_in_directory(directory_path):\n",
        "    all_raw, names = [], []\n",
        "    pdfs = [f for f in os.listdir(directory_path) if f.lower().endswith('.pdf')]\n",
        "    if not pdfs:\n",
        "        print(\"⚠️ No PDFs found.\")\n",
        "        return [], []\n",
        "    for fn in pdfs:\n",
        "        pages = []\n",
        "        try:\n",
        "            with pdfplumber.open(os.path.join(directory_path, fn)) as pdf:\n",
        "                for p in pdf.pages:\n",
        "                    # text\n",
        "                    txt = p.extract_text() or \"\"\n",
        "                    pages.append(txt)\n",
        "                    # tables\n",
        "                    for table in p.extract_tables():\n",
        "                        if table:\n",
        "                            df = pd.DataFrame(table[1:], columns=table[0]) if table[0] else pd.DataFrame(table)\n",
        "                            pages.append(df.to_csv(index=False))\n",
        "                    # OCR\n",
        "                    img = p.to_image(resolution=300).original\n",
        "                    tmp = f\"/tmp/{fn}-{p.page_number}.png\"\n",
        "                    img.save(tmp)\n",
        "                    ocr = reader.readtext(tmp, detail=0)\n",
        "                    os.remove(tmp)\n",
        "                    if ocr:\n",
        "                        pages.append(\" \".join(ocr))\n",
        "            all_raw.append(\"\\n\".join(pages))\n",
        "            names.append(fn)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {fn}: {e}\")\n",
        "    return all_raw, names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsD6YABHgC-s",
        "outputId": "1d254a4d-e812-403e-d60b-93ae76ea6ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAISS db"
      ],
      "metadata": {
        "id": "UOQhH19HgusM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load & chunk\n",
        "texts, files = extract_text_from_pdfs_in_directory(PDF_DIRECTORY)\n",
        "docs = [Document(page_content=txt, metadata={\"source\":fn}) for txt, fn in zip(texts, files)]\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(docs)\n",
        "print(f\"About to index {len(chunks)} chunks…\")\n",
        "\n",
        "# embed & save/load\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "if os.path.exists(VECTORSTORE_PATH):\n",
        "    vectorstore = FAISS.load_local(VECTORSTORE_PATH, embeddings, allow_dangerous_deserialization=True)\n",
        "else:\n",
        "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "    vectorstore.save_local(VECTORSTORE_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYV1BrqQgmwO",
        "outputId": "cb8450b3-398f-46cc-92c5-3a008a31f22c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "About to index 276 chunks…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM & RAG"
      ],
      "metadata": {
        "id": "I6baaUGwg9WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Init Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0, api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "# RAG helper\n",
        "def answer_with_rag(query: str, top_k: int = 1000) -> str:\n",
        "    # retrieve\n",
        "    docs = vectorstore.similarity_search(query, k=top_k)\n",
        "    if not docs:\n",
        "        return \"Sorry, I found no relevant shipment data.\"\n",
        "\n",
        "    # build context snippet\n",
        "    context = \"\\n\\n\".join(\n",
        "        f\"Source: {d.metadata['source']}\\n{d.page_content[:1000].replace(chr(10), ' ')}…\"\n",
        "        for d in docs\n",
        "    )\n",
        "\n",
        "    # prompt instructions + context + query\n",
        "    system = SystemMessage(content=\"You are a shipping‑document analyst.\")\n",
        "    human = HumanMessage(content=(\n",
        "        f\"CONTEXT:\\n{context}\\n\\n\"\n",
        "        f\"QUESTION: {query}\\n\\n\"\n",
        "        \"Group by CONSIGNOR, then by document type (PreAlert, NOA, POD).\"\n",
        "        \"If a value is not present in any source, write N/A.\"\n",
        "        \"Respond **exactly** in this format (no extra words):\\n\\n\"\n",
        "        \"I found <NUMBER_OF_SHIPMENTS> shipments FOR <CONSIGNOR> (from DocumentType)\\n\"\n",
        "        \"Shipment1:Reference: <Ocean Bill of lading>\\n\"\n",
        "        \"            Estimate Departing: <ETD>\\n\"\n",
        "        \"            Estimate Arriving: <ETA>\\n\"\n",
        "        \"            Actual Departing: <ATD>\\n\"\n",
        "        \"            Actual Arriving: <ATA>\\n\"\n",
        "        \"            Container#: <CONTAINER>\\n\"\n",
        "        \"            Delivered: <JobDate> at <Time Delivered>\"\n",
        "    ))\n",
        "\n",
        "    # call LLM\n",
        "    ai_message = llm.predict_messages([system, human])\n",
        "    return ai_message.content"
      ],
      "metadata": {
        "id": "GlMHnNF_g2Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query"
      ],
      "metadata": {
        "id": "FiCNcidFhSrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How many shipments are currently tracked?\"\n",
        "print(answer_with_rag(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYetNllEhLHs",
        "outputId": "e2000436-89e9-46f1-9f6d-5b6dcc6d068b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I found 10 shipments FOR CDE INTERNATIONAL (from PreAlert)\n",
            "Shipment1:Reference: 08681143591\n",
            "            Estimate Departing: 04-May-24 21:00\n",
            "            Estimate Arriving: 11-May-24 11:00\n",
            "            Actual Departing: N/A\n",
            "            Actual Arriving: N/A\n",
            "            Container#: N/A\n",
            "            Delivered: N/A at N/A\n",
            "Shipment2:Reference: HDMUSHAZ03315800\n",
            "            Estimate Departing: 16-Mar-24\n",
            "            Estimate Arriving: 05-Apr-24\n",
            "            Actual Departing: N/A\n",
            "            Actual Arriving: N/A\n",
            "            Container#: HDMU5537247\n",
            "            Delivered: N/A at N/A\n",
            "I found 2 shipments FOR SOUTH PACIFIC LOGISTICS CO.,LTD (from PreAlert)\n",
            "Shipment1:Reference: COSU6409766200\n",
            "            Estimate Departing: 19-Feb-25\n",
            "            Estimate Arriving: 06-Mar-25\n",
            "            Actual Departing: N/A\n",
            "            Actual Arriving: N/A\n",
            "            Container#: TTNU8531064\n",
            "            Delivered: N/A at N/A\n",
            "Shipment2:Reference: AMG0150149\n",
            "            Estimate Departing: 25-May-25\n",
            "            Estimate Arriving: 17-Jun-25\n",
            "            Actual Departing: N/A\n",
            "            Actual Arriving: N/A\n",
            "            Container#: FBIU5215545\n",
            "            Delivered: N/A at N/A\n",
            "I found 2 shipments FOR ABC INTERNATIONAL (SHANGHAI) CO., LTD (from PreAlert)\n",
            "Shipment1:Reference: OOLU2735833830\n",
            "            Estimate Departing: 11-Apr-24\n",
            "            Estimate Arriving: 02-May-24\n",
            "            Actual Departing: 11-Apr-24\n",
            "            Actual Arriving: N/A\n",
            "            Container#: FSCU8895133\n",
            "            Delivered: N/A at N/A\n",
            "Shipment2:Reference: OOLU2734921070\n",
            "            Estimate Departing: 23-Apr-24\n",
            "            Estimate Arriving: 05-May-24\n",
            "            Actual Departing: 23-Apr-24\n",
            "            Actual Arriving: N/A\n",
            "            Container#: TCKU6804692\n",
            "            Delivered: N/A at N/A\n",
            "I found 1 shipment FOR INFINITY CARGO EXPRESS LIMITED (from PreAlert)\n",
            "Shipment1:Reference: CAA0221669\n",
            "            Estimate Departing: 09/04/25\n",
            "            Estimate Arriving: 24/04/25\n",
            "            Actual Departing: N/A\n",
            "            Actual Arriving: N/A\n",
            "            Container#: CGMU5071817\n",
            "            Delivered: N/A at N/A\n",
            "I found 2 shipments FOR FLEXPORT INTERNATIONAL (from NOA)\n",
            "Shipment1:Reference: COSU6409766200\n",
            "            Estimate Departing: N/A\n",
            "            Estimate Arriving: 26-FEB-24\n",
            "            Actual Departing: N/A\n",
            "            Actual Arriving: N/A\n",
            "            Container#: CMAU2653673\n",
            "            Delivered: 15/11/2024 at 10:28:00\n",
            "Shipment2:Reference: ALK0444644\n",
            "            Estimate Departing: N/A\n",
            "            Estimate Arriving: 19-FEB-24\n",
            "            Actual Departing: N/A\n",
            "            Actual Arriving: N/A\n",
            "            Container#: CMAU0478782\n",
            "            Delivered: 19/11/2024 at 10:42\n",
            "I found 1 shipment FOR LOGISBER FORWARDING SL (from NOA)\n",
            "Shipment1:Reference: HKGMEL626124V\n",
            "            Estimate Departing: N/A\n",
            "            Estimate Arriving: 13-FEB-24\n",
            "            Actual Departing: N/A\n",
            "            Actual Arriving: N/A\n",
            "            Container#: TCNU7685817\n",
            "            Delivered: 18/11/2024 at 12:08:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WyoyePDehaFZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
